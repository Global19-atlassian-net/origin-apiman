# About the APIMan Components

The API Management subsystem consists of APIMan components with a
backing data store.

## ElasticSearch

ElasticSearch is a Lucene-based indexing object store which is used to
hold configuration and metrics for APIMan. The ElasticSearch cluster
should be deployed with redundancy and persistent storage for scale and
high availability.

## APIMan console

The console presents a web UI to manage API access by both API owners
and consumers. It can be reached from a link in the OpenShift console.
A REST API also runs on the same server for automated access.

## APIMan gateway

The gateway is the access point for all managed APIs. It is exposed at
an external route on the cluster and all routes are available at paths
added to the route.

## Deployer

The deployer enables the system administrator to generate all of the
necessary keys/certs/secrets and deploy all of the APIMan components
in concert.

# Using the Deployer

The deployer pod can enable deploying the full stack of the
APIMan solution with just a few prerequisites:

1. An "apiman-deployer" Secret with optional keys and certificates for the components and routes
2. An "apiman-deployer" ServiceAccount with the secret and granted privileges
3. Sufficient volumes defined for ElasticSearch cluster storage
4. A router deployment for serving cluster-defined routes

The deployer generates all the necessary certs/keys/etc for cluster
communication and defines secrets and templates for all of the necessary
API objects to implement APIMan. There are some
manual steps you must run with cluster-admin privileges in order to
grant the appropriate permissions to the related service accounts.

## Choose a Project

You may want to put all APIMan-related entities in a dedicated project.
But if you are using the multitenant SDN plugin, the APIMan
gateway can only reach the backend APIs if it is located in the `default`
project, so use that.

For examples in this document we will assume the `default` project. If your
cluster is not using the multitenant SDN plugin, then any project can be used.

## Create missing templates

If your installation did not create templates in the `openshift`
namespace, the `apiman-deployer-account-template` and `apiman-deployer-template`
templates may not exist. In that case you can create them with the following:

    $ oc apply -n openshift -f https://raw.githubusercontent.com/openshift/origin-apiman/master/deployer/deployer.yaml

You can use the same command to update the template when it changes.

## Create the Deployer Secret

Security parameters for the APIMan infrastructure
deployment can be supplied to the deployer in the form of a
[secret](https://github.com/openshift/openshift-docs/blob/master/dev_guide/secrets.adoc).

All contents of the secret are optional, but the secret itself must
always be created in order to run the deployer. To supply no parameters,
you can create an empty secret:

    $ oc secrets new apiman-deployer nothing=/dev/null

The following files may be supplied in the deployer secret, but will be
generated if not provided:

* `gateway-route.crt` - A PEM-format browser-facing certificate for the gateway server route (if not supplied, the default router cert applies).
* `gateway-route.key` - A PEM-format key to be used with the gateway route certificate.
* `console-route.crt` - A PEM-format browser-facing certificate for the console server route (if not supplied, the default router cert applies).
* `console-route.key` - A PEM-format key to be used with the console route certificate.
* `ca.key` - A CA key that will be used to sign any certificates generated by the deployer.
* `ca.crt` - A certificate for the internal component CA, used to validate all internal certificates.
* `ca.serial.txt` - A serial number file for the internal component CA.
* `.keystore`, `.keystore.password`, `.truststore`, `.truststore.password` (all must be present) appended to each component: `apiman-console`, `apiman-gateway`, `apiman-elasticsearch` - a Java keystore containing the related server key and cert, password for that keystore, a Java truststore containing the CA for validating server connections and client certs, and a key for that truststore.

An invocation supplying properly signed route certs might be:

    $ oc secrets new apiman-deployer \
       console-route.crt=/path/to/cert console-route.key=/path/to/key \
       gateway-route.crt=/path/to/cert2 gateway-route.key=/path/to/key2 \

## Create Supporting Service Accounts

The deployer must run with service accounts defined as follows:

    $ oc new-app apiman-deployer-account-template

This creates all necessary ServiceAccount and policy objects; it only
needs to be performed once, after which the deployer can be invoked
many times.

The APIMan deployment requires one service account to be given special
privileges. Run the following command to give the console access to
read projects across the cluster: (note, change `:default:` below to
the project of your choice):

    $ oadm policy add-cluster-role-to-user cluster-reader system:serviceaccount:default:apiman-console

## Run the Deployer

You will need to specify the hostname at which the gateway and console should be
exposed to client browsers, and also the master URL where client
browsers will be directed for authenticating to OpenShift. You should
read the [ElasticSearch](#elasticsearch) section below before choosing
ElasticSearch parameters for the deployer. These and other parameters
are available:

* `GATEWAY_HOSTNAME` (required): External hostname where API clients will reach the gateway
* `CONSOLE_HOSTNAME` (required): External hostname where web clients will reach the console
* `PUBLIC_MASTER_URL` (required): External URL for the master, for OAuth purposes
* `ES_CLUSTER_SIZE` (required): How many instances of ElasticSearch to deploy. At least 3 are needed for redundancy, and more can be used for scaling.
* `ES_INSTANCE_RAM`: Amount of RAM to reserve per ElasticSearch instance (e.g. 1024M, 2G). Defaults to 512M; must be at least 256M.
* `ES_PVC_SIZE`: Size of the PersistentVolumeClaim to create per ElasticSearch ops instance, e.g. 10G. If empty, no PVCs will be created and emptyDir volumes are used instead.
* `ES_PVC_PREFIX`: Prefix for the names of PersistentVolumeClaims to be created; a number will be appended per instance. If they don't already exist, they will be created with size `ES_PVC_SIZE`.
* `STORAGE_NODESELECTOR`: Specify the nodeSelector for placing Elasticsearch (label=value)
* `GATEWAY_NODESELECTOR`: Specify the nodeSelector for placing the gateway (label=value)
* `CONSOLE_NODESELECTOR`: Specify the nodeSelector for placing the console (label=value)
* `IMAGE_PREFIX`: Specify prefix for APIMan component images; e.g. for "docker.io/openshift/origin-apiman-deployer:v1.1", set prefix "docker.io/openshift/origin-"
* `IMAGE_VERSION`: Specify version for APIMan component images; e.g. for "docker.io/openshift/origin-apiman-deployer:v1.1", set version "v1.1"
* `IMAGE_PULL_SECRET`: The name of a pull secret to be used for APIMan component images
* `INSECURE_REGISTRY`: Set "true" if the registry for APIMan component images is not secured by a properly-signed certificate

You run the deployer by instantiating a template. Here is an example with some parameters:

    $ oc new-app apiman-deployer-template \
                 -p GATEWAY_HOSTNAME=gateway.example.com \
                 -p CONSOLE_HOSTNAME=console.example.com \
                 -p PUBLIC_MASTER_URL=https://localhost:8443 \
                 -p ES_CLUSTER_SIZE=1

This creates a deployer pod and prints its name. Wait until the pod
is running; this can take up to a few minutes to retrieve the deployer
image from its registry. You can watch for it with:

    $ oc get -w pod <pod-name>

If it seems to be taking too long, you can retrieve more details about the pod and
any associated events with:

    $ oc describe pod <pod-name>

When it runs, check the logs of the resulting pod (`oc logs -f <pod name>`)
for deployment progress and any errors. More details are given below.

### Scaling APIMan

You may scale the APIMan deployments normally for redundancy:

    $ oc scale dc/apiman-console --replicas=2
    $ oc scale dc/apiman-gateway --replicas=2


### ElasticSearch

The deployer creates the number of ElasticSearch instances specified by
`ES_CLUSTER_SIZE`. The nature of ElasticSearch and current Kubernetes
limitations require that we use a different scaling mechanism than the
standard Kubernetes scaling.

Scaling a standard deployment (a Kubernetes ReplicationController)
to multiple pods currently mounts the same volumes on all pods in the
deployment. However, multiple ElasticSearch instances in a cluster
cannot share storage; each pod requires its own storage. Work is under
way to enable specifying multiple volumes to be allocated individually
to instances in a deployment, but for now the deployer creates multiple
deployments in order to scale ElasticSearch to multiple instances. You
can view the deployments with:

    $ oc get dc --selector apiman-infra=elasticsearch

These deployments all have different names but will cluster with each other
via `service/apiman-es-cluster`.

It is possible to scale your cluster up after creation by adding more
deployments from a template; however, scaling up (or down) requires
the correct procedure and an awareness of clustering parameters (to be
described in a separate section). It is best if you can indicate the
desired scale at first deployment.

Refer to [Elastic's
documentation](https://www.elastic.co/guide/en/elasticsearch/guide/current/hardware.html#_disks)
for considerations involved in choosing storage and network location
as directed below.

#### Storage

By default, the deployer creates an ephemeral deployment in which all
of a pod's data will be lost any time it is restarted. For production
use you should specify a persistent storage volume for each deployment
of ElasticSearch. The deployer parameters with `PVC` in the name should
be used for this. You can either use a pre-existing set of PVCs (specify
a common prefix for their names and append numbers starting at 1, for
example with default prefix `apiman-es-` supply PVCs `apiman-es-1`,
`apiman-es-2`, etc.), or the deployer can create them with a request
for a specified size. This is the recommended method of supplying
persistent storage.

You may instead choose to add volumes manually to deployments with the
`oc volume` command. For example, to use a local directory on the host
(which is actually recommended by Elastic in order to take advantage of
local disk performance):

    $ oc volume dc/apiman-es-rca2m9u8 \
              --add --overwrite --name=elasticsearch-storage \
              --type=hostPath --path=/path/to/storage

Note: In order to allow the pods to mount host volumes, you will
need to add the `apiman-elasticsearch` service account to
the `privileged` SCC and modify the Elasticsearch DeploymentConfigs
to put the pods in the `privileged` securityContext. Use node
selectors and node labels carefully to ensure that pods land on nodes
with the storage you intend.

See `oc volume -h` for further options. E.g. if you have a specific NFS volume
you would like to use, you can set it with:

    $ oc volume dc/apiman-es-rca2m9u8 \
              --add --overwrite --name=elasticsearch-storage \
              --source='{"nfs": {"server": "nfs.server.example.com", "path": "/exported/path"}}'

#### Node selector

ElasticSearch can be very resource-heavy, particularly in RAM, depending
on the volume of logs your cluster generates. Per Elastic's guidance,
all members of the cluster should have low latency network connections
to each other.  You will likely want to direct the instances to dedicated
nodes, or a dedicated region in your cluster. You can do this by supplying
a node selector in each deployment.

The deployer has options to specify a nodeSelector label for
Elasticsearch, the gateway, and the console. If you have already deployed
the EFK stack or would like to change your current nodeSelector labels,
see below.

There is no helpful command for adding a node selector. You will need to
`oc edit` each DeploymentConfig and add the `nodeSelector` element to specify
the label corresponding to your desired nodes, e.g.:

    apiVersion: v1
    kind: DeploymentConfig
    spec:
      template:
        spec:
          nodeSelector:
            nodelabel: apiman-es-node-1

Alternatively, you can use `oc patch` to do this as well:
```
oc patch dc/apiman-es-{unique name} -p '{"spec":{"template":{"spec":{"nodeSelector":{"nodelabel":"apiman-es-node-1"}}}}}'
```

Recall that the default scheduler algorithm will spread pods to different
nodes (in the same region, if regions are defined). However this can
have unexpected consequences in several scenarios and you will most
likely want to label and specify designated nodes for ElasticSearch.

#### Settings

There are some administrative settings that can be supplied (ref. [Elastic documentation](https://www.elastic.co/guide/en/elasticsearch/guide/current/_important_configuration_changes.html)).

* `minimum_master_nodes` - the quorum required to elect a new master. Should be more than half the intended cluster size.
* `recover_after_nodes` - when restarting the cluster, require this many nodes to be present before starting recovery.
* `expected_nodes` and `recover_after_time` - when restarting the cluster, wait for number of nodes to be present or time to expire before starting recovery.

These are, respectively, the `NODE_QUORUM`, `RECOVER_AFTER_NODES`,
`RECOVER_EXPECTED_NODES`, and `RECOVER_AFTER_TIME` parameters in the
deployments and the ES template. The deployer also enables specifying
these parameters (with the `ES_` prefix), however usually its defaults
should be sufficient.
